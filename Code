###Stopword 
Pip install nltk
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

text = "decision trees are a very effective method of supervised learning.it aims is the partition of a dataset into groups as homogeneous as possible in terms of the variable to be predicted it takes as input a set of classified data, and outputs a tree that resembles to an orientation diagram where each end node leaf is a decision a class and each non final node internal represents a test. each leaf represents the decision of belonging to a class of data verifying all tests path from the root to the leaf."

nltk.download('stopwords')

tokens = word_tokenize(text)

tokens = [word.lower() for word in tokens]
#layds tu trong tieng anh
stop_words = set(stopwords.words('english'))

stopwords_count = {}
for word in tokens:
    if word in stop_words:
        if word in stopwords_count:
            stopwords_count[word] += 1
        else:
            stopwords_count[word] = 1
            
for word, count in stopwords_count.items():
    print(f"{word}: {count}")


######Wordcount
def count_word_frequency(file_path):
    word_frequency = {}
    with open(file_path, 'r') as file:
        content = file.read()
        words = content.split()
        for word in words:
            if word in word_frequency:
                word_frequency[word] += 1
            else:
                word_frequency[word] = 1
    return word_frequency

file_path = 'wordcount.txt'
word_frequency = count_word_frequency(file_path)
for word, frequency in word_frequency.items():
    print(f"Từ '{word}' xuất hiện {frequency} lần")


###Bloom filter
from pybloom_live import BloomFilter

bloom_filter = BloomFilter(capacity=1000, error_rate=0.1)

words = "decision trees are a very effective method of supervised learning.it aims is the partition of a dataset into groups as homogeneous as possible in terms of the variable to be predicted it takes as input a set of classified data, and outputs a tree that resembles to an orientation diagram where each end node leaf is a decision a class and each non final node internal represents a test. each leaf represents the decision of belonging to a class of data verifying all tests path from the root to the leaf.".split()

for word in words:
    bloom_filter.add(word)

word_to_check = "decision,44,kkk"
if word_to_check in bloom_filter:
    print(f"The word '{word_to_check}' co  ton tai.")
else:
    print(f"The word '{word_to_check}' co the khong ton tai.")

#####codelab6
import mmh3
import math

def flajolet_martin(stream):
    num_buckets = 32
    max_zeros = [0] * num_buckets

    for word in stream:
        hash_value = mmh3.hash(word)
        binary = bin(hash_value)[2:]
        num_zeros = len(binary) - len(binary.rstrip('0'))
        bucket_index = int(binary[:num_buckets][::-1], 2)

        if num_zeros > max_zeros[bucket_index]:
            max_zeros[bucket_index] = num_zeros

    distinct_elements = 2 ** max(max_zeros)
    return distinct_elements

text = "ab23 Big data is a field that treats ways to analyze from or otherwise deal with data Current usage of the term big data tends to predictive analytics user behavior analytics or advanced data analytics Data sets grow rapidly in part because they are increasingly Internet of things devices such as mobile devices Xz33 Small Small Small Small Small Small Small Small Small Small Small Small Small Small Small Small"

stream = text.split()
distinct_elements = flajolet_martin(stream)

print("Các từ riêng biệt và số lần lặp lại:")
for word in set(stream):
    count = stream.count(word)
    print(f"{word}: {count}")
    
print("Số lượng phần tử riêng biệt:", distinct_elements)
Bạn đã gửi
import hashlib

def flajolet_martin(data):
    max_rightmost_zero_bit_position = -1
    
    for element in data:
        hash_value = hashlib.md5(element.encode()).hexdigest()
        rightmost_zero_bit_position = len(hash_value) - len(hash_value.rstrip('0'))
        
        if rightmost_zero_bit_position > max_rightmost_zero_bit_position:
            max_rightmost_zero_bit_position = rightmost_zero_bit_position
    
    estimated_number_of_distinct_elements = 2 ** max_rightmost_zero_bit_position
    
    return estimated_number_of_distinct_elements

data = "ab23 Big data is a field that treats ways to analyze from or otherwise deal with data Current usage of the term big data tends to predictive analytics user behavior, analytics or advanced data analytics Data sets grow rapidly in part because they are increasingly Internet of things devices such as mobile devices. Xz33. Small Small Small Small Small Small Small Small Small Small Small Small Small Small Small Small"

elements = data.split()

estimated_count = flajolet_martin(elements)
small_count = elements.count("Small")

print("Số tên của các phần tử riêng biệt là:", estimated_count)




import mmh3
import math

def flajolet_martin(stream):
    num_buckets = 32
    max_zeros = [0] * num_buckets

    for word in stream:
        hash_value = mmh3.hash(word)
        binary = bin(hash_value)[2:]
        num_zeros = len(binary) - len(binary.rstrip('0'))
        bucket_index = int(binary[:num_buckets][::-1], 2)

        if num_zeros > max_zeros[bucket_index]:
            max_zeros[bucket_index] = num_zeros

    distinct_elements = 2 ** max(max_zeros)
    return distinct_elements

text = "ab23 Big data is a field that treats ways to analyze from or otherwise deal with data Current usage of the term big data tends to predictive analytics user behavior analytics or advanced data analytics Data sets grow rapidly in part because they are increasingly Internet of things devices such as mobile devices Xz33 Small Small Small Small Small Small Small Small Small Small Small Small Small Small Small Small"

stream = text.split()
distinct_elements = flajolet_martin(stream)

print("Các từ riêng biệt và số lần lặp lại:")
for word in set(stream):
    count = stream.count(word)
    print(f"{word}: {count}")
    
print("Số lượng phần tử riêng biệt:", distinct_elements)
